{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83436e3d",
   "metadata": {},
   "source": [
    "삽입한 링크(지정한 회사의 financial times 뉴스 메인페이지) 의 뉴스를 지정한 페이지만큼 제목과 summary 추출, -1(최대부정)~1(최대긍정)으로 값을 매긴다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60056442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 processed\n",
      "Page 2 processed\n",
      "Page 3 processed\n",
      "Page 4 processed\n",
      "Page 5 processed\n",
      "Page 6 processed\n",
      "Page 7 processed\n",
      "Page 8 processed\n",
      "Page 9 processed\n",
      "Page 10 processed\n",
      "Page 11 processed\n",
      "Page 12 processed\n",
      "Page 13 processed\n",
      "Page 14 processed\n",
      "Page 15 processed\n",
      "Page 16 processed\n",
      "Page 17 processed\n",
      "Page 18 processed\n",
      "Page 19 processed\n",
      "Page 20 processed\n",
      "Page 21 processed\n",
      "Page 22 processed\n",
      "Page 23 processed\n",
      "Page 24 processed\n",
      "Page 25 processed\n",
      "Page 26 processed\n",
      "Page 27 processed\n",
      "Page 28 processed\n",
      "Page 29 processed\n",
      "Page 30 processed\n",
      "CSV ready\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_sentiment(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = soup.select('.o-teaser__heading a')\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        # Extracting the date\n",
    "        date_element = article.find_previous(class_='stream-card__date')\n",
    "        date = date_element.find('time')['datetime']\n",
    "\n",
    "        # Extracting the year, month, and day from the date\n",
    "        year, month, day = date.split('T')[0].split('-')\n",
    "\n",
    "        # Extracting the news title\n",
    "        title = article.get_text()\n",
    "\n",
    "        # Extracting the news summary if it exists\n",
    "        summary_element = article.find_next(class_='o-teaser__standfirst')\n",
    "        summary = summary_element.find('a').get_text() if summary_element else ''\n",
    "\n",
    "        # Analyzing sentiment using nltk's SentimentIntensityAnalyzer\n",
    "        sentiment_scores = analyzer.polarity_scores(title + ', ' + summary)\n",
    "        sentiment = sentiment_scores['compound']\n",
    "\n",
    "        results.append({'Year': year, 'Month': month, 'Day': day, 'Title': title, 'Summary': summary, 'Sentiment': sentiment})\n",
    "\n",
    "    return results\n",
    "\n",
    "base_url = 'https://www.ft.com/stream/e621710a-7d0e-4081-a8cc-76669a303f1f?page='\n",
    "num_pages = 30\n",
    "\n",
    "all_results = []\n",
    "for page in range(1, num_pages + 1):\n",
    "    url = base_url + str(page)\n",
    "    results = analyze_sentiment(url)\n",
    "    all_results.extend(results)\n",
    "    print(f\"Page {page} processed\")\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv('sentiment_analysis_SaudiAramco.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "print('CSV ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aba753d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 processed\n",
      "Page 2 processed\n",
      "Page 3 processed\n",
      "Page 4 processed\n",
      "Page 5 processed\n",
      "Page 6 processed\n",
      "Page 7 processed\n",
      "Page 8 processed\n",
      "Page 9 processed\n",
      "Page 10 processed\n",
      "Page 11 processed\n",
      "Page 12 processed\n",
      "Page 13 processed\n",
      "Page 14 processed\n",
      "Page 15 processed\n",
      "Page 16 processed\n",
      "Page 17 processed\n",
      "Page 18 processed\n",
      "Page 19 processed\n",
      "Page 20 processed\n",
      "Page 21 processed\n",
      "Page 22 processed\n",
      "Page 23 processed\n",
      "Page 24 processed\n",
      "Page 25 processed\n",
      "Page 26 processed\n",
      "Page 27 processed\n",
      "Page 28 processed\n",
      "Page 29 processed\n",
      "Page 30 processed\n",
      "CSV ready\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_sentiment(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = soup.select('.o-teaser__heading a')\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        # Extracting the date\n",
    "        date_element = article.find_previous(class_='stream-card__date')\n",
    "        date = date_element.find('time')['datetime']\n",
    "\n",
    "        # Extracting the year, month, and day from the date\n",
    "        year, month, day = date.split('T')[0].split('-')\n",
    "\n",
    "        # Extracting the news title\n",
    "        title = article.get_text()\n",
    "\n",
    "        # Extracting the news summary if it exists\n",
    "        summary_element = article.find_next(class_='o-teaser__standfirst')\n",
    "        summary = summary_element.find('a').get_text() if summary_element else ''\n",
    "\n",
    "        # Analyzing sentiment using nltk's SentimentIntensityAnalyzer\n",
    "        sentiment_scores = analyzer.polarity_scores(title + ', ' + summary)\n",
    "        sentiment = sentiment_scores['compound']\n",
    "\n",
    "        results.append({'Year': year, 'Month': month, 'Day': day, 'Title': title, 'Summary': summary, 'Sentiment': sentiment})\n",
    "\n",
    "    return results\n",
    "\n",
    "base_url = 'https://www.ft.com/stream/35edec46-ef7b-4f9b-b85a-25174e6e07fa?page='\n",
    "num_pages = 30\n",
    "\n",
    "all_results = []\n",
    "for page in range(1, num_pages + 1):\n",
    "    url = base_url + str(page)\n",
    "    results = analyze_sentiment(url)\n",
    "    all_results.extend(results)\n",
    "    print(f\"Page {page} processed\")\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv('sentiment_analysis_Tesla.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "print('CSV ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "032767d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 processed\n",
      "Page 2 processed\n",
      "Page 3 processed\n",
      "Page 4 processed\n",
      "Page 5 processed\n",
      "Page 6 processed\n",
      "Page 7 processed\n",
      "Page 8 processed\n",
      "Page 9 processed\n",
      "Page 10 processed\n",
      "Page 11 processed\n",
      "Page 12 processed\n",
      "Page 13 processed\n",
      "Page 14 processed\n",
      "Page 15 processed\n",
      "Page 16 processed\n",
      "Page 17 processed\n",
      "Page 18 processed\n",
      "Page 19 processed\n",
      "Page 20 processed\n",
      "Page 21 processed\n",
      "Page 22 processed\n",
      "Page 23 processed\n",
      "Page 24 processed\n",
      "Page 25 processed\n",
      "Page 26 processed\n",
      "Page 27 processed\n",
      "Page 28 processed\n",
      "Page 29 processed\n",
      "Page 30 processed\n",
      "CSV ready\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_sentiment(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = soup.select('.o-teaser__heading a')\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        # Extracting the date\n",
    "        date_element = article.find_previous(class_='stream-card__date')\n",
    "        date = date_element.find('time')['datetime']\n",
    "\n",
    "        # Extracting the year, month, and day from the date\n",
    "        year, month, day = date.split('T')[0].split('-')\n",
    "\n",
    "        # Extracting the news title\n",
    "        title = article.get_text()\n",
    "\n",
    "        # Extracting the news summary if it exists\n",
    "        summary_element = article.find_next(class_='o-teaser__standfirst')\n",
    "        summary = summary_element.find('a').get_text() if summary_element else ''\n",
    "\n",
    "        # Analyzing sentiment using nltk's SentimentIntensityAnalyzer\n",
    "        sentiment_scores = analyzer.polarity_scores(title + ', ' + summary)\n",
    "        sentiment = sentiment_scores['compound']\n",
    "\n",
    "        results.append({'Year': year, 'Month': month, 'Day': day, 'Title': title, 'Summary': summary, 'Sentiment': sentiment})\n",
    "\n",
    "    return results\n",
    "\n",
    "base_url = 'https://www.ft.com/nvidia?page='\n",
    "num_pages = 30\n",
    "\n",
    "all_results = []\n",
    "for page in range(1, num_pages + 1):\n",
    "    url = base_url + str(page)\n",
    "    results = analyze_sentiment(url)\n",
    "    all_results.extend(results)\n",
    "    print(f\"Page {page} processed\")\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv('sentiment_analysis_Nvidia.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "print('CSV ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "aa3d1bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 processed\n",
      "Page 2 processed\n",
      "Page 3 processed\n",
      "Page 4 processed\n",
      "Page 5 processed\n",
      "Page 6 processed\n",
      "Page 7 processed\n",
      "Page 8 processed\n",
      "Page 9 processed\n",
      "Page 10 processed\n",
      "Page 11 processed\n",
      "Page 12 processed\n",
      "Page 13 processed\n",
      "Page 14 processed\n",
      "Page 15 processed\n",
      "Page 16 processed\n",
      "Page 17 processed\n",
      "Page 18 processed\n",
      "Page 19 processed\n",
      "Page 20 processed\n",
      "Page 21 processed\n",
      "Page 22 processed\n",
      "Page 23 processed\n",
      "Page 24 processed\n",
      "Page 25 processed\n",
      "Page 26 processed\n",
      "Page 27 processed\n",
      "Page 28 processed\n",
      "Page 29 processed\n",
      "Page 30 processed\n",
      "CSV ready\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_sentiment(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = soup.select('.o-teaser__heading a')\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        # Extracting the date\n",
    "        date_element = article.find_previous(class_='stream-card__date')\n",
    "        date = date_element.find('time')['datetime']\n",
    "\n",
    "        # Extracting the year, month, and day from the date\n",
    "        year, month, day = date.split('T')[0].split('-')\n",
    "\n",
    "        # Extracting the news title\n",
    "        title = article.get_text()\n",
    "\n",
    "        # Extracting the news summary if it exists\n",
    "        summary_element = article.find_next(class_='o-teaser__standfirst')\n",
    "        summary = summary_element.find('a').get_text() if summary_element else ''\n",
    "\n",
    "        # Analyzing sentiment using nltk's SentimentIntensityAnalyzer\n",
    "        sentiment_scores = analyzer.polarity_scores(title + ', ' + summary)\n",
    "        sentiment = sentiment_scores['compound']\n",
    "\n",
    "        results.append({'Year': year, 'Month': month, 'Day': day, 'Title': title, 'Summary': summary, 'Sentiment': sentiment})\n",
    "\n",
    "    return results\n",
    "\n",
    "base_url = 'https://www.ft.com/stream/e1028fc7-6cb5-4215-a550-77ce239c5f79?page='\n",
    "num_pages = 30\n",
    "\n",
    "all_results = []\n",
    "for page in range(1, num_pages + 1):\n",
    "    url = base_url + str(page)\n",
    "    results = analyze_sentiment(url)\n",
    "    all_results.extend(results)\n",
    "    print(f\"Page {page} processed\")\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv('sentiment_analysis_BerkshireHathaway.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "print('CSV ready')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "947c9f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 processed\n",
      "Page 2 processed\n",
      "Page 3 processed\n",
      "Page 4 processed\n",
      "Page 5 processed\n",
      "Page 6 processed\n",
      "Page 7 processed\n",
      "Page 8 processed\n",
      "Page 9 processed\n",
      "Page 10 processed\n",
      "Page 11 processed\n",
      "Page 12 processed\n",
      "Page 13 processed\n",
      "Page 14 processed\n",
      "Page 15 processed\n",
      "Page 16 processed\n",
      "Page 17 processed\n",
      "Page 18 processed\n",
      "Page 19 processed\n",
      "Page 20 processed\n",
      "Page 21 processed\n",
      "Page 22 processed\n",
      "Page 23 processed\n",
      "Page 24 processed\n",
      "Page 25 processed\n",
      "Page 26 processed\n",
      "Page 27 processed\n",
      "Page 28 processed\n",
      "Page 29 processed\n",
      "Page 30 processed\n",
      "CSV ready\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_sentiment(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    articles = soup.select('.o-teaser__heading a')\n",
    "\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "    results = []\n",
    "    for article in articles:\n",
    "        # Extracting the date\n",
    "        date_element = article.find_previous(class_='stream-card__date')\n",
    "        date = date_element.find('time')['datetime']\n",
    "\n",
    "        # Extracting the year, month, and day from the date\n",
    "        year, month, day = date.split('T')[0].split('-')\n",
    "\n",
    "        # Extracting the news title\n",
    "        title = article.get_text()\n",
    "\n",
    "        # Extracting the news summary if it exists\n",
    "        summary_element = article.find_next(class_='o-teaser__standfirst')\n",
    "        summary = summary_element.find('a').get_text() if summary_element else ''\n",
    "\n",
    "        # Analyzing sentiment using nltk's SentimentIntensityAnalyzer\n",
    "        sentiment_scores = analyzer.polarity_scores(title + ', ' + summary)\n",
    "        sentiment = sentiment_scores['compound']\n",
    "\n",
    "        results.append({'Year': year, 'Month': month, 'Day': day, 'Title': title, 'Summary': summary, 'Sentiment': sentiment})\n",
    "\n",
    "    return results\n",
    "\n",
    "base_url = 'https://www.ft.com/stream/a39a4558-f562-4dca-8774-000246e6eebe?page='\n",
    "num_pages = 30\n",
    "\n",
    "all_results = []\n",
    "for page in range(1, num_pages + 1):\n",
    "    url = base_url + str(page)\n",
    "    results = analyze_sentiment(url)\n",
    "    all_results.extend(results)\n",
    "    print(f\"Page {page} processed\")\n",
    "\n",
    "df = pd.DataFrame(all_results)\n",
    "df.to_csv('sentiment_analysis_Apple.csv', encoding='utf-8-sig', index=False)\n",
    "\n",
    "print('CSV ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffbb176",
   "metadata": {},
   "source": [
    "---같은 날의 뉴스들의 sentiment 값을 모두 더해 일별로 total sentiment를 구한다---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "897818d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output CSV file created: output_sentiment_sum_Apple.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the path for the input CSV file\n",
    "input_csv_file = 'sentiment_analysis_Apple.csv'\n",
    "\n",
    "# Create a dictionary to store the sum of sentiment per date\n",
    "sentiment_sum = defaultdict(float)\n",
    "\n",
    "# Read the input CSV file and calculate the sum of sentiment for each date\n",
    "with open(input_csv_file, 'r', encoding='utf-8-sig') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    next(reader)  # Skip the header row if it exists\n",
    "\n",
    "    for row in reader:\n",
    "        year = int(row[0])\n",
    "        month = int(row[1])\n",
    "        day = int(row[2])\n",
    "        date = datetime(year, month, day).date()\n",
    "        title = row[3]\n",
    "        summary = row[4]\n",
    "        sentiment = float(row[5])\n",
    "\n",
    "        if date >= datetime.strptime('2019-05-01', '%Y-%m-%d').date() and date <= datetime.strptime('2023-05-01', '%Y-%m-%d').date():\n",
    "            sentiment_sum[date] += sentiment\n",
    "\n",
    "# Create a new CSV file for the output\n",
    "output_csv_file = 'output_sentiment_sum_Apple.csv'\n",
    "with open(output_csv_file, 'w', newline='', encoding='utf-8-sig') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['Date', 'SentimentSum'])  # Write the header row\n",
    "\n",
    "    for date, sentiment in sentiment_sum.items():\n",
    "        writer.writerow([date, sentiment])\n",
    "\n",
    "print(f\"Output CSV file created: {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b3cf46d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output CSV file created: output_sentiment_sum_BerkshireHathaway.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the path for the input CSV file\n",
    "input_csv_file = 'sentiment_analysis_BerkshireHathaway.csv'\n",
    "\n",
    "# Create a dictionary to store the sum of sentiment per date\n",
    "sentiment_sum = defaultdict(float)\n",
    "\n",
    "# Read the input CSV file and calculate the sum of sentiment for each date\n",
    "with open(input_csv_file, 'r', encoding='utf-8-sig') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    next(reader)  # Skip the header row if it exists\n",
    "\n",
    "    for row in reader:\n",
    "        year = int(row[0])\n",
    "        month = int(row[1])\n",
    "        day = int(row[2])\n",
    "        date = datetime(year, month, day).date()\n",
    "        title = row[3]\n",
    "        summary = row[4]\n",
    "        sentiment = float(row[5])\n",
    "\n",
    "        if date >= datetime.strptime('2019-05-01', '%Y-%m-%d').date() and date <= datetime.strptime('2023-05-01', '%Y-%m-%d').date():\n",
    "            sentiment_sum[date] += sentiment\n",
    "\n",
    "# Create a new CSV file for the output\n",
    "output_csv_file = 'output_sentiment_sum_BerkshireHathaway.csv'\n",
    "with open(output_csv_file, 'w', newline='', encoding='utf-8-sig') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['Date', 'SentimentSum'])  # Write the header row\n",
    "\n",
    "    for date, sentiment in sentiment_sum.items():\n",
    "        writer.writerow([date, sentiment])\n",
    "\n",
    "print(f\"Output CSV file created: {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd74895",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output CSV file created: output_sentiment_sum_Nvidia.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the path for the input CSV file\n",
    "input_csv_file = 'sentiment_analysis_Nvidia.csv'\n",
    "\n",
    "# Create a dictionary to store the sum of sentiment per date\n",
    "sentiment_sum = defaultdict(float)\n",
    "\n",
    "# Read the input CSV file and calculate the sum of sentiment for each date\n",
    "with open(input_csv_file, 'r', encoding='utf-8-sig') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    next(reader)  # Skip the header row if it exists\n",
    "\n",
    "    for row in reader:\n",
    "        year = int(row[0])\n",
    "        month = int(row[1])\n",
    "        day = int(row[2])\n",
    "        date = datetime(year, month, day).date()\n",
    "        title = row[3]\n",
    "        summary = row[4]\n",
    "        sentiment = float(row[5])\n",
    "\n",
    "        if date >= datetime.strptime('2019-05-01', '%Y-%m-%d').date() and date <= datetime.strptime('2023-05-01', '%Y-%m-%d').date():\n",
    "            sentiment_sum[date] += sentiment\n",
    "\n",
    "# Create a new CSV file for the output\n",
    "output_csv_file = 'output_sentiment_sum_Nvidia.csv'\n",
    "with open(output_csv_file, 'w', newline='', encoding='utf-8-sig') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['Date', 'SentimentSum'])  # Write the header row\n",
    "\n",
    "    for date, sentiment in sentiment_sum.items():\n",
    "        writer.writerow([date, sentiment])\n",
    "\n",
    "print(f\"Output CSV file created: {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0baae9a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output CSV file created: output_sentiment_sum_Saudiaramco.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the path for the input CSV file\n",
    "input_csv_file = 'sentiment_analysis_SaudiAramco.csv'\n",
    "\n",
    "# Create a dictionary to store the sum of sentiment per date\n",
    "sentiment_sum = defaultdict(float)\n",
    "\n",
    "# Read the input CSV file and calculate the sum of sentiment for each date\n",
    "with open(input_csv_file, 'r', encoding='utf-8-sig') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    next(reader)  # Skip the header row if it exists\n",
    "\n",
    "    for row in reader:\n",
    "        year = int(row[0])\n",
    "        month = int(row[1])\n",
    "        day = int(row[2])\n",
    "        date = datetime(year, month, day).date()\n",
    "        title = row[3]\n",
    "        summary = row[4]\n",
    "        sentiment = float(row[5])\n",
    "\n",
    "        if date >= datetime.strptime('2019-05-01', '%Y-%m-%d').date() and date <= datetime.strptime('2023-05-01', '%Y-%m-%d').date():\n",
    "            sentiment_sum[date] += sentiment\n",
    "\n",
    "# Create a new CSV file for the output\n",
    "output_csv_file = 'output_sentiment_sum_Saudiaramco.csv'\n",
    "with open(output_csv_file, 'w', newline='', encoding='utf-8-sig') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['Date', 'SentimentSum'])  # Write the header row\n",
    "\n",
    "    for date, sentiment in sentiment_sum.items():\n",
    "        writer.writerow([date, sentiment])\n",
    "\n",
    "print(f\"Output CSV file created: {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6977cbf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output CSV file created: output_sentiment_sum_Tesla.csv\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from collections import defaultdict\n",
    "from datetime import datetime\n",
    "\n",
    "# Define the path for the input CSV file\n",
    "input_csv_file = 'sentiment_analysis_Tesla.csv'\n",
    "\n",
    "# Create a dictionary to store the sum of sentiment per date\n",
    "sentiment_sum = defaultdict(float)\n",
    "\n",
    "# Read the input CSV file and calculate the sum of sentiment for each date\n",
    "with open(input_csv_file, 'r', encoding='utf-8-sig') as input_file:\n",
    "    reader = csv.reader(input_file)\n",
    "    next(reader)  # Skip the header row if it exists\n",
    "\n",
    "    for row in reader:\n",
    "        year = int(row[0])\n",
    "        month = int(row[1])\n",
    "        day = int(row[2])\n",
    "        date = datetime(year, month, day).date()\n",
    "        title = row[3]\n",
    "        summary = row[4]\n",
    "        sentiment = float(row[5])\n",
    "\n",
    "        if date >= datetime.strptime('2019-05-01', '%Y-%m-%d').date() and date <= datetime.strptime('2023-05-01', '%Y-%m-%d').date():\n",
    "            sentiment_sum[date] += sentiment\n",
    "\n",
    "# Create a new CSV file for the output\n",
    "output_csv_file = 'output_sentiment_sum_Tesla.csv'\n",
    "with open(output_csv_file, 'w', newline='', encoding='utf-8-sig') as output_file:\n",
    "    writer = csv.writer(output_file)\n",
    "    writer.writerow(['Date', 'SentimentSum'])  # Write the header row\n",
    "\n",
    "    for date, sentiment in sentiment_sum.items():\n",
    "        writer.writerow([date, sentiment])\n",
    "\n",
    "print(f\"Output CSV file created: {output_csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6383b98",
   "metadata": {},
   "source": [
    "여기는 그냥 엑셀파일 합치는 코드입니다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "540f835a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Read data from file A\n",
    "data_A = []\n",
    "with open('AAPL Historical Data.csv', 'r', encoding='utf-8-sig') as file_A:\n",
    "    reader_A = csv.DictReader(file_A)\n",
    "    for row in reader_A:\n",
    "        date_str = row['Date']\n",
    "        date = datetime.strptime(date_str, '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "        row['Date'] = date\n",
    "        data_A.append(row)\n",
    "\n",
    "# Read data from file B and store sentiment sum values in a dictionary\n",
    "data_B = {}\n",
    "with open('output_sentiment_sum_Apple.csv', 'r', encoding='utf-8-sig') as file_B:\n",
    "    reader_B = csv.DictReader(file_B)\n",
    "    for row in reader_B:\n",
    "        date_str = row['Date']\n",
    "        sentiment_sum = float(row['SentimentSum'])\n",
    "        data_B[date_str] = sentiment_sum\n",
    "\n",
    "# Update data in file A with sentiment sum values from file B\n",
    "for row in data_A:\n",
    "    date = row['Date']\n",
    "    if date in data_B:\n",
    "        row['SentimentSum'] = data_B[date]\n",
    "    else:\n",
    "        row['SentimentSum'] = 0\n",
    "\n",
    "# Write updated data to a new CSV file\n",
    "fieldnames = data_A[0].keys()\n",
    "with open('output_merged_Apple.csv', 'w', newline='', encoding='utf-8-sig') as merged_file:\n",
    "    writer = csv.DictWriter(merged_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c3dc35ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Read data from file A\n",
    "data_A = []\n",
    "with open('TSLA Historical Data.csv', 'r', encoding='utf-8-sig') as file_A:\n",
    "    reader_A = csv.DictReader(file_A)\n",
    "    for row in reader_A:\n",
    "        date_str = row['Date']\n",
    "        date = datetime.strptime(date_str, '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "        row['Date'] = date\n",
    "        data_A.append(row)\n",
    "\n",
    "# Read data from file B and store sentiment sum values in a dictionary\n",
    "data_B = {}\n",
    "with open('output_sentiment_sum_Tesla.csv', 'r', encoding='utf-8-sig') as file_B:\n",
    "    reader_B = csv.DictReader(file_B)\n",
    "    for row in reader_B:\n",
    "        date_str = row['Date']\n",
    "        sentiment_sum = float(row['SentimentSum'])\n",
    "        data_B[date_str] = sentiment_sum\n",
    "\n",
    "# Update data in file A with sentiment sum values from file B\n",
    "for row in data_A:\n",
    "    date = row['Date']\n",
    "    if date in data_B:\n",
    "        row['SentimentSum'] = data_B[date]\n",
    "    else:\n",
    "        row['SentimentSum'] = 0\n",
    "\n",
    "# Write updated data to a new CSV file\n",
    "fieldnames = data_A[0].keys()\n",
    "with open('output_merged_Tesla.csv', 'w', newline='', encoding='utf-8-sig') as merged_file:\n",
    "    writer = csv.DictWriter(merged_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "55aff845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Read data from file A\n",
    "data_A = []\n",
    "with open('NVDA Historical Data.csv', 'r', encoding='utf-8-sig') as file_A:\n",
    "    reader_A = csv.DictReader(file_A)\n",
    "    for row in reader_A:\n",
    "        date_str = row['Date']\n",
    "        date = datetime.strptime(date_str, '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "        row['Date'] = date\n",
    "        data_A.append(row)\n",
    "\n",
    "# Read data from file B and store sentiment sum values in a dictionary\n",
    "data_B = {}\n",
    "with open('output_sentiment_sum_Nvidia.csv', 'r', encoding='utf-8-sig') as file_B:\n",
    "    reader_B = csv.DictReader(file_B)\n",
    "    for row in reader_B:\n",
    "        date_str = row['Date']\n",
    "        sentiment_sum = float(row['SentimentSum'])\n",
    "        data_B[date_str] = sentiment_sum\n",
    "\n",
    "# Update data in file A with sentiment sum values from file B\n",
    "for row in data_A:\n",
    "    date = row['Date']\n",
    "    if date in data_B:\n",
    "        row['SentimentSum'] = data_B[date]\n",
    "    else:\n",
    "        row['SentimentSum'] = 0\n",
    "\n",
    "# Write updated data to a new CSV file\n",
    "fieldnames = data_A[0].keys()\n",
    "with open('output_merged_Nvidia.csv', 'w', newline='', encoding='utf-8-sig') as merged_file:\n",
    "    writer = csv.DictWriter(merged_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "212162fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "# Read data from file A\n",
    "data_A = []\n",
    "with open('BRKb Historical Data.csv', 'r', encoding='utf-8-sig') as file_A:\n",
    "    reader_A = csv.DictReader(file_A)\n",
    "    for row in reader_A:\n",
    "        date_str = row['Date']\n",
    "        date = datetime.strptime(date_str, '%m/%d/%Y').strftime('%Y-%m-%d')\n",
    "        row['Date'] = date\n",
    "        data_A.append(row)\n",
    "\n",
    "# Read data from file B and store sentiment sum values in a dictionary\n",
    "data_B = {}\n",
    "with open('output_sentiment_sum_BerkshireHathaway.csv', 'r', encoding='utf-8-sig') as file_B:\n",
    "    reader_B = csv.DictReader(file_B)\n",
    "    for row in reader_B:\n",
    "        date_str = row['Date']\n",
    "        sentiment_sum = float(row['SentimentSum'])\n",
    "        data_B[date_str] = sentiment_sum\n",
    "\n",
    "# Update data in file A with sentiment sum values from file B\n",
    "for row in data_A:\n",
    "    date = row['Date']\n",
    "    if date in data_B:\n",
    "        row['SentimentSum'] = data_B[date]\n",
    "    else:\n",
    "        row['SentimentSum'] = 0\n",
    "\n",
    "# Write updated data to a new CSV file\n",
    "fieldnames = data_A[0].keys()\n",
    "with open('output_merged_BerkshireHathaway.csv', 'w', newline='', encoding='utf-8-sig') as merged_file:\n",
    "    writer = csv.DictWriter(merged_file, fieldnames=fieldnames)\n",
    "    writer.writeheader()\n",
    "    writer.writerows(data_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0100007a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
